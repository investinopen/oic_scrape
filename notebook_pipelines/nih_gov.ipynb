{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National Institutes of Health\n",
    "\n",
    "Data from the National Institutes of Health via their [RePORTER](https://projectreporter.nih.gov/reporter.cfm) system.\n",
    "\n",
    "Data is obtained via their RePORTER [API](https://api.reporter.nih.gov/). This API can be updated on a weekly basis (Monday mornings around 10am EST) to retrieve the latest data. When weekly updates are applied following the initial data backfill, this can be extremely up-to-date.\n",
    "\n",
    "[Data Dictionary](https://api.reporter.nih.gov/documents/Data%20Elements%20for%20RePORTER%20Project%20API_V2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "import requests\n",
    "import warnings\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from requests_cache import CachedSession\n",
    "import duckdb\n",
    "from oic_scrape.items import AwardItem, AwardParticipant\n",
    "import polars as pl\n",
    "import dateutil\n",
    "from attrs import asdict\n",
    "\n",
    "# Test if we are in a notebook, load right tqdm\n",
    "try:\n",
    "    get_ipython()  # type: ignore\n",
    "    from tqdm.notebook import tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# minimum start date is 2011-01-01 when using date_added api field\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "OUTPUT_LOCATION = \"data/nih.gov_grants.jsonl\"\n",
    "\n",
    "## NEVER COMMIT WITH THIS SET TO TRUE\n",
    "USE_CACHE = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Clean-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "# Validate and convert START_DATE\n",
    "try:\n",
    "    parameters[\"start_date\"] = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "except ValueError:\n",
    "    raise ValueError(\"Invalid START_DATE format. Please use the format 'YYYY-MM-DD'.\")\n",
    "if parameters[\"start_date\"] < datetime.datetime(2009, 1, 1).date():\n",
    "    raise ValueError(\"START_DATE is too early. Please use a date after 2009-01-01.\")\n",
    "\n",
    "# Validate and convert END_DATE\n",
    "try:\n",
    "    parameters[\"end_date\"] = datetime.datetime.strptime(END_DATE, \"%Y-%m-%d\").date()\n",
    "except ValueError:\n",
    "    raise ValueError(\"Invalid END_DATE format. Please use the format 'YYYY-MM-DD'.\")\n",
    "\n",
    "# Ensure END DATE > START_DATE\n",
    "if parameters[\"end_date\"] <= parameters[\"start_date\"]:\n",
    "    raise ValueError(\"END_DATE must be greater than START_DATE.\")\n",
    "\n",
    "\n",
    "# Validate and convert USE_CACHE\n",
    "if USE_CACHE.lower() not in [\"true\", \"false\"]:\n",
    "    raise ValueError(\"Invalid USE_CACHE value. Please use 'True' or 'False'.\")\n",
    "parameters[\"use_cache\"] = USE_CACHE.lower() == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HTTP Configuration\n",
    "\n",
    "# Cache for development use only\n",
    "if parameters[\"use_cache\"]:\n",
    "    session = CachedSession(\n",
    "        \"cache.sqlite\",\n",
    "        backend=\"sqlite\",\n",
    "        allowable_methods=(\"GET\", \"POST\"),\n",
    "        allowable_codes=(200, 404),\n",
    "    )\n",
    "else:\n",
    "    session = requests.Session()\n",
    "retry = Retry(connect=5, backoff_factor=1.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIH API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NihApi:\n",
    "    API_URL: str = \"https://api.reporter.nih.gov/v2/projects/search\"\n",
    "    last_request_timestamp: Optional[datetime.datetime] = None\n",
    "    follow_rate_limit: bool = True\n",
    "\n",
    "    def __init__(self, follow_rate_limit: bool = True):\n",
    "        self.follow_rate_limit = follow_rate_limit\n",
    "\n",
    "    def projects_search(\n",
    "        self,\n",
    "        payload: Dict[Any, Any],\n",
    "        offset: Optional[int] = None,\n",
    "        limit: Optional[int] = None,\n",
    "    ) -> Dict[Any, Any]:\n",
    "        \"\"\"\n",
    "        Calls the NIH RePORTER API to get project data.\n",
    "\n",
    "        Args:\n",
    "            payload (Dict[Any, Any]): Allows for manual specification of the complete search payload.\n",
    "                Documentation at https://api.reporter.nih.gov/?urls.primaryName=V2.0\n",
    "            offset (int, optional): The offset for the API request. Defaults to None (0). Maximum is 14,999.\n",
    "            limit (int, optional): The maximum number of results to return. Min = 1, Max = 500\n",
    "                Defaults to 50 (when not specified), but is more efficient at 500.\n",
    "\n",
    "        Returns:\n",
    "            Dict[Any, Any]: The response from the API request.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the API request fails or returns an error.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check for overlaps between the payload dictionary and convenience parameters.\n",
    "        if offset is not None and \"offset\" in payload:\n",
    "            raise ValueError(\n",
    "                \"offset is set in both payload and as a convenience parameter\"\n",
    "            )\n",
    "        if limit is not None and \"limit\" in payload:\n",
    "            raise ValueError(\n",
    "                \"limit is set in both payload and as a convenience parameter\"\n",
    "            )\n",
    "\n",
    "        if offset:\n",
    "            payload[\"offset\"] = offset\n",
    "        if limit:\n",
    "            payload[\"limit\"] = limit\n",
    "\n",
    "        # Rate limit handling: wait until 1 second has passed since the last request\n",
    "        if self.follow_rate_limit and self.last_request_timestamp:\n",
    "            time_since_last_request = (\n",
    "                datetime.datetime.utcnow() - self.last_request_timestamp\n",
    "            )\n",
    "            while time_since_last_request < timedelta(seconds=1):\n",
    "                time_since_last_request = (\n",
    "                    datetime.datetime.utcnow() - self.last_request_timestamp\n",
    "                )\n",
    "                pass\n",
    "\n",
    "        response = session.post(self.API_URL, json=payload)\n",
    "\n",
    "        # Response timestamping, with carveout for more rapid calls if previously cached\n",
    "        res_ts = datetime.datetime.utcnow()\n",
    "        if parameters[\"use_cache\"]:\n",
    "            if response.from_cache:\n",
    "                pass\n",
    "            else:\n",
    "                self.last_request_timestamp = res_ts\n",
    "        else:\n",
    "            self.last_request_timestamp = res_ts\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(\n",
    "                f\"API request failed with status code {response.status_code}. Error message: {response.text}\"\n",
    "            )\n",
    "\n",
    "        response_json = response.json()\n",
    "        if \"error\" in response_json:\n",
    "            raise ValueError(\n",
    "                f\"API request returned an error: {response_json['error']}.\"\n",
    "            )\n",
    "\n",
    "        return response_json\n",
    "\n",
    "    def get_projects_by_day(\n",
    "        self,\n",
    "        date: datetime.date,\n",
    "        datefield: str = \"date_added\",\n",
    "        add_timestamp_field: bool = False,\n",
    "        added_timestamp_field_name: str = \"_record_crawled_at\",\n",
    "        excessive_results_strategy=\"error\",\n",
    "        progress_bar: bool = False,\n",
    "    ) -> List[Dict[Any, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieves projects added on a specific day.\n",
    "\n",
    "        Args:\n",
    "            date (datetime.date): The date to retrieve projects for.\n",
    "            datefield (str, optional): The date field to search on. Defaults to \"date_added\".\n",
    "            add_timestamp_field (bool): When true, will add a field to each record with the timestamp of the crawl. Field is named with `added_timestamp_field_name`\n",
    "            added_timestamp_field_name (str):  When add_timestamp_field == True, this is the name of the new field added to each project\n",
    "            excessive_results_strategy (str, optional): The strategy to use when the number of results exceeds the maximum. Defaults to 'error'. Options are 'error', 'warn', and 'ignore'. 'warn' and 'ignore' will retrieve up to 15000 results but will not be able to retrieve results beyond that limit.\n",
    "            progress_bar (bool, optional): Whether to display a progress bar. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[Any, Any]]: The list of projects added on the specified day.\n",
    "        \"\"\"\n",
    "\n",
    "        projects = []\n",
    "        result_page_length = 500\n",
    "        max_results = 14999  # NIH RePORTER API constraint\n",
    "        sort_order = \"asc\"\n",
    "\n",
    "        payload = {\n",
    "            \"criteria\": {\n",
    "                datefield: {\n",
    "                    \"from_date\": date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"to_date\": (date + timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
    "                }\n",
    "            },\n",
    "            \"sort_field\": datefield,\n",
    "            \"sort_order\": sort_order,\n",
    "        }\n",
    "\n",
    "        # Get the metadata for the day's results\n",
    "        # Used for determining excessive results and optional tqdm progress bar\n",
    "        info_response = self.projects_search(payload, limit=1)\n",
    "        total_results = info_response[\"meta\"][\"total\"]\n",
    "\n",
    "        # Check for excessive results and handle according to selected strategy\n",
    "        # Example days for excessive results:\n",
    "        ## 2014-09-12: 18,160 results\n",
    "        if total_results > max_results:\n",
    "            if excessive_results_strategy == \"error\":\n",
    "                raise ValueError(\n",
    "                    f\"Number of results ({total_results}) exceeds the maximum ({max_results}) on {date}.\"\n",
    "                )\n",
    "            elif excessive_results_strategy == \"warn\":\n",
    "                warnings.warn(\n",
    "                    f\"Number of results ({total_results}) exceeds the maximum ({max_results}) on {date}.\"\n",
    "                )\n",
    "                # TODO: You are NOT elegantly failing when you hit the limit. Instead it just tries to exceed 15000 and fails.\n",
    "            elif excessive_results_strategy == \"ignore\":\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid excessive_results_strategy value: {excessive_results_strategy}.\"\n",
    "                )\n",
    "\n",
    "        if total_results > 0:\n",
    "            if progress_bar:\n",
    "                pbar = tqdm(\n",
    "                    total=total_results, desc=f\"Retrieving NIH projects for {date}\"\n",
    "                )\n",
    "\n",
    "            offset = 0\n",
    "            while len(projects) < total_results:\n",
    "                payload[\"offset\"] = offset\n",
    "                payload[\"limit\"] = result_page_length\n",
    "                response = self.projects_search(payload=payload)\n",
    "                response_ts = datetime.datetime.utcnow()\n",
    "                response_projects = response[\"results\"]\n",
    "\n",
    "                if add_timestamp_field:\n",
    "                    # use list comprehension\n",
    "                    response_projects = [\n",
    "                        {**project, added_timestamp_field_name: response_ts}\n",
    "                        for project in response_projects\n",
    "                    ]\n",
    "\n",
    "                n_results = len(response_projects)\n",
    "                projects.extend(response_projects)\n",
    "                offset += n_results\n",
    "                if progress_bar:\n",
    "                    pbar.update(n_results)\n",
    "\n",
    "            if progress_bar:\n",
    "                pbar.close()\n",
    "\n",
    "        if len(projects) != total_results:\n",
    "            raise ValueError(\n",
    "                f\"Number of projects retrieved ({len(projects)}) does not match the total number of projects ({total_results}) for {date}.\"\n",
    "            )\n",
    "\n",
    "        return projects\n",
    "\n",
    "    def count_projects_added_in_date_range(\n",
    "        self,\n",
    "        start_date: datetime.date,\n",
    "        end_date: datetime.date,\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of projects added in a date range.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime.date): The date to start counting projects for.\n",
    "            end_date (datetime.date): The date to stop counting projects for.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of projects added in the specified date range.\n",
    "        \"\"\"\n",
    "\n",
    "        payload = {\n",
    "            \"criteria\": {\n",
    "                \"date_added\": {\n",
    "                    \"from_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"to_date\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "                }\n",
    "            },\n",
    "            \"sort_field\": \"date_added\",\n",
    "            \"sort_order\": \"asc\",\n",
    "        }\n",
    "\n",
    "        response = self.projects_search(payload, limit=1)\n",
    "        return response[\"meta\"][\"total\"]\n",
    "\n",
    "    def get_projects_added_in_date_range(\n",
    "        self,\n",
    "        start_date: datetime.date,\n",
    "        end_date: datetime.date,\n",
    "        progress_bar=False,\n",
    "        daily_progress_bar=False,\n",
    "    ) -> List[Dict[Any, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieves projects added on a specific day.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime.date): The date to start retrieving projects for.\n",
    "            end_date (datetime.date): The date to stop retrieving projects for.\n",
    "            progress_bar (bool, optional): Whether to display a progress bar. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[Any, Any]]: The list of projects added on the specified day.\n",
    "        \"\"\"\n",
    "\n",
    "        projects: List[Dict[Any, Any]] = []\n",
    "        total_projects = self.count_projects_added_in_date_range(start_date, end_date)\n",
    "        if progress_bar:\n",
    "            pbar = tqdm(\n",
    "                total=total_projects,\n",
    "                desc=f\"Retrieving Projects for {start_date} to {end_date}\",\n",
    "            )\n",
    "        date_spine = [\n",
    "            start_date + timedelta(days=x) for x in range((end_date - start_date).days)\n",
    "        ]\n",
    "        for date in date_spine:\n",
    "            projects_on_date = self.get_projects_by_day(\n",
    "                date,\n",
    "                add_timestamp_field=True,\n",
    "                progress_bar=daily_progress_bar,\n",
    "                excessive_results_strategy=\"warn\",\n",
    "            )\n",
    "            projects.extend(projects_on_date)\n",
    "            if progress_bar:\n",
    "                pbar.update(len(projects_on_date))\n",
    "        if progress_bar:\n",
    "            pbar.close()\n",
    "\n",
    "        if len(projects) != total_projects:\n",
    "            raise ValueError(\n",
    "                f\"Number of projects retrieved ({len(projects)}) does not match the total number of projects ({total_projects}) for the date range {start_date} to {end_date}.\"\n",
    "            )\n",
    "        return projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategies for dealing with too many records:\n",
    "When the number of records is too lage, the API will throw an error (get the error). It has a limit of 500 records per request and 15000 (via offset 14999) from a search, though it'll tell the absolute number of records.\n",
    "\n",
    "##### Strategy 1: < 30,000: Sort by ascending & descending. When results = full count stop. Check for uniqueness.\n",
    "\n",
    "##### Strategy 2: >= 30,000: Use another date field as a secondary filter.\n",
    "\n",
    "If the count is greater than 15,000, include an additional date field (e.g. `project_start_date`) as part of the filtering criteria, in addition to `date_added`.  Get the minimum and maximum values for the date field by making 2 API requests of limit=1, sort ascending and descending.\n",
    "\n",
    "Begin pulling all records for each year (starting with min_date -> JAN1 of next year) by returning results with limit=500. If a set of results > 15,000 for the year, discard the first result set and subdivide the year into months and pull from MONTH, 1 to MONTH+1, 1. Subdivide to days if > 14,999 for a month.\n",
    "\n",
    "For the last results, we'll want to make sure we can project into the furute ans so would want to start only with a from_date and not a to_date. However if that's also over 15,000, we'll need a way to forward project.\n",
    "\n",
    "Be sure to test this using the date_added results > 15,000 days (as discovered below)\n",
    "\n",
    "TODO validate that we can filter `{ \"criteria\": {\"award_notice_date\":{ \"from_date\":\"2021-02-17\", \"to_date\":\"2022-02-17\" } }}` on only one of these criteria (e.g. a min date but not a max / visa versa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional NIH Data\n",
    "We need to get the data for the NIH's agencies IDs to correspond to their ROR ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNDER_NAME = \"National Institutes of Health\"\n",
    "FUNDER_ROR_ID = \"https://ror.org/01cwqze88\"\n",
    "NIH_IC_AGENCY = [\n",
    "    {\n",
    "        \"acronym\": \"CC\",\n",
    "        \"full_name\": \"Clinical Center\",\n",
    "        \"org_code\": \"CC\",\n",
    "        \"payload_criteria_value\": \"CLC\",\n",
    "        \"ror_id\": \"https://ror.org/04vfsmv21\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"CSR\",\n",
    "        \"full_name\": \"Center for Scientific Review\",\n",
    "        \"org_code\": \"RG\",\n",
    "        \"payload_criteria_value\": \"CSR\",\n",
    "        \"ror_id\": \"https://ror.org/04r5s4b52\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"CIT\",\n",
    "        \"full_name\": \"Center for Information Technology\",\n",
    "        \"org_code\": \"CIT\",\n",
    "        \"payload_criteria_value\": \"CIT\",\n",
    "        \"ror_id\": \"https://ror.org/03jh5a977\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"FIC\",\n",
    "        \"full_name\": \"John E. Fogarty International Center\",\n",
    "        \"org_code\": \"TW\",\n",
    "        \"payload_criteria_value\": \"FIC\",\n",
    "        \"ror_id\": \"https://ror.org/02xey9a22\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NCATS\",\n",
    "        \"full_name\": \"National Center for Advancing Translational Sciences (NCATS)\",\n",
    "        \"org_code\": \"TR\",\n",
    "        \"payload_criteria_value\": \"NCATS\",\n",
    "        \"ror_id\": \"https://ror.org/04pw6fb54\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NCCIH\",\n",
    "        \"full_name\": \"National Center for Complementary and Integrative Health\",\n",
    "        \"org_code\": \"AT\",\n",
    "        \"payload_criteria_value\": \"NCCIH\",\n",
    "        \"ror_id\": \"https://ror.org/00190t495\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NCI\",\n",
    "        \"full_name\": \"National Cancer Institute\",\n",
    "        \"org_code\": \"CA\",\n",
    "        \"payload_criteria_value\": \"NCI\",\n",
    "        \"ror_id\": \"https://ror.org/040gcmg81\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NCRR\",\n",
    "        \"full_name\": \"National Center for Research Resources (dissolved 12/2011)\",\n",
    "        \"org_code\": \"RR\",\n",
    "        \"payload_criteria_value\": \"NCRR\",\n",
    "        \"ror_id\": \"https://ror.org/01cwqze88\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NEI\",\n",
    "        \"full_name\": \"National Eye Institute\",\n",
    "        \"org_code\": \"EY\",\n",
    "        \"payload_criteria_value\": \"NEI\",\n",
    "        \"ror_id\": \"https://ror.org/03wkg3b53\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NHGRI\",\n",
    "        \"full_name\": \"National Human Genome Research Institute\",\n",
    "        \"org_code\": \"HG\",\n",
    "        \"payload_criteria_value\": \"NHGRI\",\n",
    "        \"ror_id\": \"https://ror.org/00baak391\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NHLBI\",\n",
    "        \"full_name\": \"National Heart, Lung, and Blood Institute\",\n",
    "        \"org_code\": \"HL\",\n",
    "        \"payload_criteria_value\": \"NHLBI\",\n",
    "        \"ror_id\": \"https://ror.org/012pb6c26\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIA\",\n",
    "        \"full_name\": \"National Institute on Aging\",\n",
    "        \"org_code\": \"AG\",\n",
    "        \"payload_criteria_value\": \"NIA\",\n",
    "        \"ror_id\": \"https://ror.org/049v75w11\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIAAA\",\n",
    "        \"full_name\": \"National Institute on Alcohol Abuse and Alcoholism\",\n",
    "        \"org_code\": \"AA\",\n",
    "        \"payload_criteria_value\": \"NIAAA\",\n",
    "        \"ror_id\": \"https://ror.org/02jzrsm59\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIAID\",\n",
    "        \"full_name\": \"National Institute of Allergy and Infectious Diseases\",\n",
    "        \"org_code\": \"AI\",\n",
    "        \"payload_criteria_value\": \"NIAID\",\n",
    "        \"ror_id\": \"https://ror.org/043z4tv69\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIAMS\",\n",
    "        \"full_name\": \"National Institute of Arthritis and Musculoskeletal and Skin Diseases\",\n",
    "        \"org_code\": \"AR\",\n",
    "        \"payload_criteria_value\": \"NIAMS\",\n",
    "        \"ror_id\": \"https://ror.org/006zn3t30\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIBIB\",\n",
    "        \"full_name\": \"National Institute of Biomedical Imaging and Bioengineering\",\n",
    "        \"org_code\": \"EB\",\n",
    "        \"payload_criteria_value\": \"NIBIB\",\n",
    "        \"ror_id\": \"https://ror.org/00372qc85\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NICHD\",\n",
    "        \"full_name\": \"Eunice Kennedy Shriver National Institute of Child Health and Human Development\",\n",
    "        \"org_code\": \"HD\",\n",
    "        \"payload_criteria_value\": \"NICHD\",\n",
    "        \"ror_id\": \"https://ror.org/04byxyr05\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIDA\",\n",
    "        \"full_name\": \"National Institute on Drug Abuse\",\n",
    "        \"org_code\": \"DA\",\n",
    "        \"payload_criteria_value\": \"NIDA\",\n",
    "        \"ror_id\": \"https://ror.org/00fq5cm18\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIDCD\",\n",
    "        \"full_name\": \"National Institute on Deafness and Other Communication Disorders\",\n",
    "        \"org_code\": \"DC\",\n",
    "        \"payload_criteria_value\": \"NIDCD\",\n",
    "        \"ror_id\": \"https://ror.org/04mhx6838\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIDCR\",\n",
    "        \"full_name\": \"National Institute of Dental and Craniofacial Research\",\n",
    "        \"org_code\": \"DE\",\n",
    "        \"payload_criteria_value\": \"NIDCR\",\n",
    "        \"ror_id\": \"https://ror.org/004a2wv92\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIDDK\",\n",
    "        \"full_name\": \"National Institute of Diabetes and Digestive and Kidney Diseases\",\n",
    "        \"org_code\": \"DK\",\n",
    "        \"payload_criteria_value\": \"NIDDK\",\n",
    "        \"ror_id\": \"https://ror.org/00adh9b73\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIEHS\",\n",
    "        \"full_name\": \"National Institute of Environmental Health Sciences\",\n",
    "        \"org_code\": \"ES\",\n",
    "        \"payload_criteria_value\": \"NIEHS\",\n",
    "        \"ror_id\": \"https://ror.org/00j4k1h63\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIGMS\",\n",
    "        \"full_name\": \"National Institute of General Medical Sciences\",\n",
    "        \"org_code\": \"GM\",\n",
    "        \"payload_criteria_value\": \"NIGMS\",\n",
    "        \"ror_id\": \"https://ror.org/04q48ey07\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIMH\",\n",
    "        \"full_name\": \"National Institute of Mental Health\",\n",
    "        \"org_code\": \"MH\",\n",
    "        \"payload_criteria_value\": \"NIMH\",\n",
    "        \"ror_id\": \"https://ror.org/04xeg9z08\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NIMHD\",\n",
    "        \"full_name\": \"National Institute on Minority Health and Health Disparities\",\n",
    "        \"org_code\": \"MD\",\n",
    "        \"payload_criteria_value\": \"NIMHD\",\n",
    "        \"ror_id\": \"https://ror.org/0493hgw16\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NINDS\",\n",
    "        \"full_name\": \"National Institute of Neurological Disorders and Stroke\",\n",
    "        \"org_code\": \"NS\",\n",
    "        \"payload_criteria_value\": \"NINDS\",\n",
    "        \"ror_id\": \"https://ror.org/01s5ya894\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NINR\",\n",
    "        \"full_name\": \"National Institute of Nursing Research\",\n",
    "        \"org_code\": \"NR\",\n",
    "        \"payload_criteria_value\": \"NINR\",\n",
    "        \"ror_id\": \"https://ror.org/01y3zfr79\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"NLM\",\n",
    "        \"full_name\": \"National Library of Medicine\",\n",
    "        \"org_code\": \"LM\",\n",
    "        \"payload_criteria_value\": \"NLM\",\n",
    "        \"ror_id\": \"https://ror.org/0060t0j89\",\n",
    "    },\n",
    "    {\n",
    "        \"acronym\": \"OD\",\n",
    "        \"full_name\": \"Office of the Director\",\n",
    "        \"org_code\": \"OD\",\n",
    "        \"payload_criteria_value\": \"OD\",\n",
    "        \"ror_id\": \"https://ror.org/00fj8a872\",\n",
    "    },\n",
    "]\n",
    "\n",
    "lookup_agency = {agency[\"org_code\"]: agency for agency in NIH_IC_AGENCY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NIH API Analysis:\n",
    "\n",
    "Determine the number of records we're crawling on a daily basis. We can embed this as a graphic, but mostly this is just useful analysis to understand if we need more advanced crawling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_projects_by_date(\n",
    "    start_date: datetime.date, end_date: datetime.date\n",
    ") -> Dict[datetime.date, int]:\n",
    "    nih = NihApi()\n",
    "\n",
    "    # count the number of projects added for each date from START_DATE to END_DATE\n",
    "\n",
    "    date_spine = [\n",
    "        parameters[\"start_date\"] + timedelta(days=x)\n",
    "        for x in range((parameters[\"end_date\"] - parameters[\"start_date\"]).days)\n",
    "    ]\n",
    "\n",
    "    # records_added_daily_count = {date: nih.count_projects_added_in_date_range(date, date + timedelta(days=1)) for date in date_spine}\n",
    "\n",
    "    records_added_daily_count = {}\n",
    "    for date in tqdm(date_spine, total=len(date_spine)):\n",
    "        try:\n",
    "            today_count = nih.count_projects_added_in_date_range(\n",
    "                date, date + timedelta(days=1)\n",
    "            )\n",
    "            records_added_daily_count[date] = today_count\n",
    "            if today_count > 14999:\n",
    "                print(f\"Warning: {today_count} records added on {date}.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            records_added_daily_count[date] = 0\n",
    "    return records_added_daily_count\n",
    "\n",
    "\n",
    "# records_added_daily_count = count_projects_by_date(parameters['start_date'], parameters['end_date'])\n",
    "# count_daily_records = pd.DataFrame(records_added_daily_count.items(), columns=[\"date\", \"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nih = NihApi()\n",
    "projects = nih.get_projects_added_in_date_range(\n",
    "    parameters[\"start_date\"], parameters[\"end_date\"], progress_bar=True, daily_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df = pd.json_normalize(projects, max_level=2)\n",
    "del projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set types for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [\n",
    "    \"project_start_date\",\n",
    "    \"project_end_date\",\n",
    "    \"award_notice_date\",\n",
    "    \"budget_start\",\n",
    "    \"budget_end\",\n",
    "    \"date_added\",\n",
    "]\n",
    "\n",
    "for col in date_columns:\n",
    "    projects_df[col] = pd.to_datetime(projects_df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ror_id(code):\n",
    "    \"\"\"\n",
    "    Retrieves the ROR ID associated with the given NIH agency code.\n",
    "\n",
    "    Args:\n",
    "        code (str): The code to look up.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The ROR ID if found, None otherwise.\n",
    "    \"\"\"\n",
    "    return lookup_agency[code][\"ror_id\"] if code in lookup_agency else None\n",
    "\n",
    "\n",
    "## delete if we do away with Pandas\n",
    "def build_location(row):\n",
    "    \"\"\"\n",
    "    Builds a location string based on the provided row data.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series): The row containing organization information.\n",
    "\n",
    "    Returns:\n",
    "        str: The location string.\n",
    "    \"\"\"\n",
    "    location = \"\"\n",
    "    if pd.notnull(row[\"organization.org_city\"]):\n",
    "        location += f\"{row['organization.org_city']}, \"\n",
    "    elif pd.notnull(row[\"organization.city\"]):\n",
    "        location += f\"{row['organization.city']}, \"\n",
    "    if pd.notnull(row[\"organization.org_zipcode\"]):\n",
    "        # handle zip-5/zip-9 formats\n",
    "        if row[\"organization.org_zipcode\"].isnumeric():\n",
    "            if len(row[\"organization.org_zipcode\"]) == 5:\n",
    "                location += f\"{row['organization.org_zipcode']} ,\"\n",
    "            elif len(row[\"organization.org_zipcode\"]) == 9:\n",
    "                location += f\"{row['organization.org_zipcode'][:5]}-{row['organization.org_zipcode'][5:]}, \"\n",
    "        else:\n",
    "            location += f\"{row['organization.org_zipcode']}, \"\n",
    "    if pd.notnull(row[\"organization.org_country\"]):\n",
    "        location += row[\"organization.org_country\"]\n",
    "    elif pd.notnull(row[\"organization.country\"]):\n",
    "        location += row[\"organization.country\"]\n",
    "    return location\n",
    "\n",
    "\n",
    "def extract_format_pi_names(pi_list) -> str:\n",
    "    return \" | \".join(pi[\"full_name\"] for pi in pi_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df[\"grant_id\"] = \"nih::\" + projects_df[\"appl_id\"].astype(str)\n",
    "export_df[\"funder_name\"] = \"National Institutes of Health: \" + projects_df[\n",
    "    \"agency_ic_admin.name\"\n",
    "].astype(str)\n",
    "export_df[\"funder_ror_id\"] = projects_df[\"agency_ic_admin.code\"].apply(get_ror_id)\n",
    "export_df[\"recipient_org_name\"] = projects_df[\"organization.org_name\"]\n",
    "export_df[\"recipient_location\"] = projects_df.apply(build_location, axis=1)\n",
    "\n",
    "export_df[\"grant_year\"] = projects_df[\"budget_start\"].dt.year\n",
    "export_df[\"grant_duration\"] = (\n",
    "    projects_df[\"budget_end\"].dt.date - projects_df[\"budget_start\"].dt.date\n",
    ")\n",
    "export_df[\"grant_start_date\"] = projects_df[\"budget_start\"].dt.date\n",
    "export_df[\"grant_end_date\"] = projects_df[\"budget_end\"].dt.date\n",
    "\n",
    "export_df[\"award_amount\"] = projects_df[\"award_amount\"]\n",
    "export_df[\"award_currency\"] = \"USD\"\n",
    "export_df[\"award_amount_usd\"] = projects_df[\"award_amount\"]\n",
    "export_df[\"source\"] = \"NIH RePORTER API\"\n",
    "export_df[\"source_url\"] = \"https://api.reporter.nih.gov/?urls.primaryName=V2.0\"\n",
    "export_df[\"grant_title\"] = projects_df[\"project_title\"]\n",
    "export_df[\"grant_description\"] = (\n",
    "    \"PROJECT TITLE: \"\n",
    "    + projects_df[\"project_title\"]\n",
    "    + \"\\n\\n\\n PROJECT ABSTRACT:\\n\"\n",
    "    + projects_df[\"abstract_text\"]\n",
    "    + \"\\n\\n\\n\"\n",
    "    + \"PUBLIC HEALTH RELEVANCE STATEMENT: \\n\"\n",
    "    + projects_df[\"phr_text\"]\n",
    ")\n",
    "export_df[\"grant_category\"] = projects_df[\"funding_mechanism\"]\n",
    "\n",
    "## Notify the user if the record represents a subproject\n",
    "subproject_comment = \"Grant record is for a subproject. Value reflected here is value of the subproject only. Parent grant has cumulative value of funding of all subprojects. If summed, this value may be counted twice if using the overall dataset. Use the project_id in the NIH's raw_source_data if you would like to identify the parent project.\"\n",
    "\n",
    "export_df[\"comment\"] = projects_df[\"subproject_id\"].where(\n",
    "    projects_df[\"subproject_id\"].isna(), subproject_comment\n",
    ")\n",
    "\n",
    "export_df[\"_crawled_at\"] = projects_df[\"_record_crawled_at\"]\n",
    "export_df[\"raw_export_data\"] = projects_df.drop(\"_record_crawled_at\", axis=1).to_dict(\n",
    "    orient=\"records\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del projects_df\n",
    "export_df.to_json(OUTPUT_LOCATION, orient=\"records\", lines=True, date_format=\"iso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the file so it will fit\n",
    "! split -C 95M --numeric-suffixes --additional-suffix .jsonl {OUTPUT_LOCATION} {OUTPUT_LOCATION.split('.jsonl')[0]}_part_\n",
    "\n",
    "# Remove the original file so oversize file doesn't get committed\n",
    "! rm {OUTPUT_LOCATION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/nih.gov_grants_part_00.jsonl  data/nih.gov_grants_part_14.jsonl\n",
      "data/nih.gov_grants_part_01.jsonl  data/nih.gov_grants_part_15.jsonl\n",
      "data/nih.gov_grants_part_02.jsonl  data/nih.gov_grants_part_16.jsonl\n",
      "data/nih.gov_grants_part_03.jsonl  data/nih.gov_grants_part_17.jsonl\n",
      "data/nih.gov_grants_part_04.jsonl  data/nih.gov_grants_part_18.jsonl\n",
      "data/nih.gov_grants_part_05.jsonl  data/nih.gov_grants_part_19.jsonl\n",
      "data/nih.gov_grants_part_06.jsonl  data/nih.gov_grants_part_20.jsonl\n",
      "data/nih.gov_grants_part_07.jsonl  data/nih.gov_grants_part_21.jsonl\n",
      "data/nih.gov_grants_part_08.jsonl  data/nih.gov_grants_part_22.jsonl\n",
      "data/nih.gov_grants_part_09.jsonl  data/nih.gov_grants_part_23.jsonl\n",
      "data/nih.gov_grants_part_10.jsonl  data/nih.gov_grants_part_24.jsonl\n",
      "data/nih.gov_grants_part_11.jsonl  data/nih.gov_grants_part_25.jsonl\n",
      "data/nih.gov_grants_part_12.jsonl  data/nih.gov_grants_part_26.jsonl\n",
      "data/nih.gov_grants_part_13.jsonl\n"
     ]
    }
   ],
   "source": [
    "! ls {OUTPUT_LOCATION.split('.jsonl')[0]}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data on a Daily Basis; use out-of-memory processing\n",
    "\n",
    "We've run into memory issues while doing this all in Pandas. Instead, we will:\n",
    "1. do the data transformation on each record as we get it (daily)\n",
    "2. Put each record into AwardItem objects\n",
    "3. Turn a whole day's AwardItems into a Polars dataframe\n",
    "4. Write that Polars dataframe to an on-disk DuckDB database\n",
    "5. Allow the Garbage Collector to free the dict and the Dataframe (or do it ourselves)\n",
    "6. Once complete, we can have DuckDB export the records to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning for building the AwardItem\n",
    "\n",
    "def build_clean_location(project: dict[Any, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Builds a location string based on the project's various location fields data.\n",
    "\n",
    "    Args:\n",
    "        project: one NIH Project entity from the NihAPI.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted location string.\n",
    "    \"\"\"\n",
    "    location = \"\"\n",
    "    organization = project.get(\"organization\", {})\n",
    "    if organization.get(\"org_city\"):\n",
    "        location += f\"{organization['org_city']}, \"\n",
    "    elif organization.get(\"city\"):\n",
    "        location += f\"{organization['city']}, \"\n",
    "    if organization.get(\"org_state\"):\n",
    "        location += f\"{organization['org_state']}, \"\n",
    "    if organization.get(\"org_zipcode\"):\n",
    "        # handle zip-5/zip-9 formats\n",
    "        zipcode = organization[\"org_zipcode\"]\n",
    "        if zipcode.isnumeric():\n",
    "            if len(zipcode) == 5:\n",
    "                location += f\"{zipcode} ,\"\n",
    "            elif len(zipcode) == 9:\n",
    "                location += f\"{zipcode[:5]}-{zipcode[5:]}, \"\n",
    "        else:\n",
    "            location += f\"{zipcode}, \"\n",
    "    if organization.get(\"org_country\"):\n",
    "        location += organization[\"org_country\"]\n",
    "    elif organization.get(\"country\"):\n",
    "        location += organization[\"country\"]\n",
    "    return location\n",
    "\n",
    "def get_ror_id(code):\n",
    "    \"\"\"\n",
    "    Retrieves the ROR ID associated with the given NIH agency code.\n",
    "\n",
    "    Args:\n",
    "        code (str): The code to look up.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The ROR ID if found, None otherwise.\n",
    "    \"\"\"\n",
    "    return lookup_agency[code][\"ror_id\"] if code in lookup_agency else None\n",
    "\n",
    "subproject_comment = \"Grant record is for a subproject. Value reflected here is value of the subproject only. Parent grant has cumulative value of funding of all subprojects. If summed, this value may be counted twice if using the overall dataset. Use the project_id in the NIH's raw_source_data if you would like to identify the parent project.\"\n",
    "\n",
    "def structure_award_item(project: dict[Any, Any]) -> AwardItem:\n",
    "    \"\"\"\n",
    "    Structures the AwardItem from the NIH project data.\n",
    "\n",
    "    Args:\n",
    "        project: one NIH Project entity from the NihAPI.\n",
    "\n",
    "    Returns:\n",
    "        AwardItem: The structured AwardItem.\n",
    "    \"\"\" \n",
    "\n",
    "    # Build the most complex thing: the PI List:\n",
    "    pi_list = []\n",
    "    for pi in project[\"principal_investigators\"]:\n",
    "        pi_list.append(\n",
    "            AwardParticipant(\n",
    "                full_name=\" \".join(\n",
    "                    name\n",
    "                    for name in (pi[\"first_name\"], pi[\"middle_name\"], pi[\"last_name\"])\n",
    "                    if name\n",
    "                ),\n",
    "                is_pi=pi[\"is_contact_pi\"],\n",
    "                grant_role=\"Contact PI\" if pi[\"is_contact_pi\"] else \"PI\",\n",
    "                first_name=pi[\"first_name\"],\n",
    "                middle_name=pi[\"middle_name\"],\n",
    "                last_name=pi[\"last_name\"],\n",
    "                identifiers={\"nih_profile_id\": str(pi[\"profile_id\"])},\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    # Build the components of the AwardItem\n",
    "    crawled_at = project[\"_record_crawled_at\"]\n",
    "    del project['_record_crawled_at'] # remove added field for export\n",
    "\n",
    "    source_data = str(project)\n",
    "    grant_start_date = dateutil.parser.parse(project['budget_start']) if project.get('budget_start') else None\n",
    "    grant_end_date = dateutil.parser.parse(project['budget_end']) if project.get('budget_end') else None\n",
    "    \n",
    "    ai = AwardItem(\n",
    "            _crawled_at = crawled_at,\n",
    "            source=\"nih.gov\",\n",
    "            grant_id = f\"nih::{project['appl_id']}\",\n",
    "            funder_org_name = f\"National Institutes of Health: {project['agency_ic_admin']['name']}\" if project.get('agency_ic_admin') else \"National Institutes of Health\",\n",
    "            funder_org_ror_id = get_ror_id(project['agency_ic_admin']['code']) if project['agency_ic_admin'] else FUNDER_ROR_ID,\n",
    "            recipient_org_name = project['organization']['org_name'] if project['organization'].get('org_name') else \"__Organization Unknown__\",\n",
    "            recipient_org_location = build_clean_location(project),\n",
    "            pi_name = next((person.full_name for person in pi_list if person.grant_role == 'Contact PI'), None),\n",
    "            named_participants=pi_list,\n",
    "            grant_start_date = grant_start_date,\n",
    "            grant_end_date =grant_end_date,\n",
    "            grant_year = grant_start_date.year if grant_start_date else None,\n",
    "            grant_duration = str(grant_end_date - grant_start_date) if grant_start_date and grant_end_date else None,\n",
    "            award_amount = float(project['award_amount']) if project.get('award_amount') else None,\n",
    "            award_currency = \"USD\",\n",
    "            award_amount_usd = float(project['award_amount']) if project.get('award_amount') else None,\n",
    "            source_url = project['project_detail_url'],\n",
    "            grant_title = project['project_title'],\n",
    "            grant_description = f\"PROJECT TITLE: {project['project_title']}\\n\\nPROJECT ABSTRACT:\\n{project['abstract_text']}\\n\\nPUBLIC HEALTH RELEVANCE STATEMENT: \\n{project['phr_text']}\",\n",
    "            program_of_funder = project['funding_mechanism'],\n",
    "            comments = subproject_comment if project.get('subproject_id') else None,\n",
    "            raw_source_data = source_data,\n",
    "            _award_schema_version = \"0.1.0\"\n",
    "    )\n",
    "    return ai\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21f368b973f4dc8a956b5c74ab2f5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Projects from 2015-01-01 to 2024-03-01:   0%|          | 0/724407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving projects for 2015-01-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving projects for 2015-01-02\n",
      "Retrieving projects for 2015-01-03\n",
      "Retrieving projects for 2015-01-04\n",
      "Retrieving projects for 2015-01-05\n",
      "Retrieving projects for 2015-01-06\n",
      "Retrieving projects for 2015-01-07\n",
      "Retrieving projects for 2015-01-08\n",
      "Retrieving projects for 2015-01-09\n",
      "Retrieving projects for 2015-01-10\n",
      "Retrieving projects for 2015-01-11\n",
      "Retrieving projects for 2015-01-12\n",
      "Retrieving projects for 2015-01-13\n",
      "Retrieving projects for 2015-01-14\n",
      "Retrieving projects for 2015-01-15\n",
      "Retrieving projects for 2015-01-16\n",
      "Retrieving projects for 2015-01-17\n",
      "Retrieving projects for 2015-01-18\n",
      "Retrieving projects for 2015-01-19\n",
      "Retrieving projects for 2015-01-20\n",
      "Retrieving projects for 2015-01-21\n",
      "Retrieving projects for 2015-01-22\n",
      "Retrieving projects for 2015-01-23\n",
      "Retrieving projects for 2015-01-24\n",
      "Retrieving projects for 2015-01-25\n",
      "Retrieving projects for 2015-01-26\n",
      "Retrieving projects for 2015-01-27\n",
      "Retrieving projects for 2015-01-28\n",
      "Retrieving projects for 2015-01-29\n",
      "Retrieving projects for 2015-01-30\n",
      "Retrieving projects for 2015-01-31\n",
      "Retrieving projects for 2015-02-01\n",
      "Retrieving projects for 2015-02-02\n",
      "Retrieving projects for 2015-02-03\n",
      "Retrieving projects for 2015-02-04\n",
      "Retrieving projects for 2015-02-05\n",
      "Retrieving projects for 2015-02-06\n",
      "Retrieving projects for 2015-02-07\n",
      "Retrieving projects for 2015-02-08\n",
      "Retrieving projects for 2015-02-09\n",
      "Retrieving projects for 2015-02-10\n",
      "Retrieving projects for 2015-02-11\n",
      "Retrieving projects for 2015-02-12\n",
      "Retrieving projects for 2015-02-13\n",
      "Retrieving projects for 2015-02-14\n",
      "Retrieving projects for 2015-02-15\n",
      "Retrieving projects for 2015-02-16\n",
      "Retrieving projects for 2015-02-17\n",
      "Retrieving projects for 2015-02-18\n",
      "Retrieving projects for 2015-02-19\n",
      "Retrieving projects for 2015-02-20\n",
      "Retrieving projects for 2015-02-21\n",
      "Retrieving projects for 2015-02-22\n",
      "Retrieving projects for 2015-02-23\n",
      "Retrieving projects for 2015-02-24\n",
      "Retrieving projects for 2015-02-25\n",
      "Retrieving projects for 2015-02-26\n",
      "Retrieving projects for 2015-02-27\n",
      "Retrieving projects for 2015-02-28\n",
      "Retrieving projects for 2015-03-01\n",
      "Retrieving projects for 2015-03-02\n",
      "Retrieving projects for 2015-03-03\n",
      "Retrieving projects for 2015-03-04\n",
      "Retrieving projects for 2015-03-05\n",
      "Retrieving projects for 2015-03-06\n",
      "Retrieving projects for 2015-03-07\n",
      "Retrieving projects for 2015-03-08\n",
      "Retrieving projects for 2015-03-09\n",
      "Retrieving projects for 2015-03-10\n",
      "Retrieving projects for 2015-03-11\n",
      "Retrieving projects for 2015-03-12\n",
      "Retrieving projects for 2015-03-13\n",
      "Retrieving projects for 2015-03-14\n",
      "Retrieving projects for 2015-03-15\n",
      "Retrieving projects for 2015-03-16\n",
      "Retrieving projects for 2015-03-17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     ai \u001b[38;5;241m=\u001b[39m structure_award_item(project)\n\u001b[1;32m     21\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     futuredf\u001b[38;5;241m.\u001b[39mappend(\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mai\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     result_list \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mLazyFrame(futuredf)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/oic-scrape-D2fotO-x-py3.11/lib/python3.11/site-packages/attr/_next_gen.py:211\u001b[0m, in \u001b[0;36masdict\u001b[0;34m(inst, recurse, filter, value_serializer)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masdict\u001b[39m(inst, \u001b[38;5;241m*\u001b[39m, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, value_serializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    Same as `attr.asdict`, except that collections types are always retained\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    and dict is always used as *dict_factory*.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 21.3.0\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_asdict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecurse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_serializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_serializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_collection_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/oic-scrape-D2fotO-x-py3.11/lib/python3.11/site-packages/attr/_funcs.py:75\u001b[0m, in \u001b[0;36masdict\u001b[0;34m(inst, recurse, filter, dict_factory, retain_collection_types, value_serializer)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mfrozenset\u001b[39m)):\n\u001b[1;32m     74\u001b[0m     cf \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_collection_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m\n\u001b[0;32m---> 75\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_asdict_anything\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdict_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretain_collection_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_collection_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue_serializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_serializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         rv[a\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m cf(items)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/oic-scrape-D2fotO-x-py3.11/lib/python3.11/site-packages/attr/_funcs.py:76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mfrozenset\u001b[39m)):\n\u001b[1;32m     74\u001b[0m     cf \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_collection_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m\n\u001b[1;32m     75\u001b[0m     items \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 76\u001b[0m         \u001b[43m_asdict_anything\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdict_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretain_collection_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_collection_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue_serializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_serializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m v\n\u001b[1;32m     85\u001b[0m     ]\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         rv[a\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m cf(items)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/oic-scrape-D2fotO-x-py3.11/lib/python3.11/site-packages/attr/_funcs.py:137\u001b[0m, in \u001b[0;36m_asdict_anything\u001b[0;34m(val, is_key, filter, dict_factory, retain_collection_types, value_serializer)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m``asdict`` only works on attrs instances, this works on anything.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(val\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__attrs_attrs__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Attrs class.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdict_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_collection_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_collection_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_serializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_serializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mfrozenset\u001b[39m)):\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retain_collection_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/oic-scrape-D2fotO-x-py3.11/lib/python3.11/site-packages/attr/_funcs.py:96\u001b[0m, in \u001b[0;36masdict\u001b[0;34m(inst, recurse, filter, dict_factory, retain_collection_types, value_serializer)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     95\u001b[0m     df \u001b[38;5;241m=\u001b[39m dict_factory\n\u001b[0;32m---> 96\u001b[0m     rv[a\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m df(\n\u001b[1;32m     97\u001b[0m         (\n\u001b[1;32m     98\u001b[0m             _asdict_anything(\n\u001b[1;32m     99\u001b[0m                 kk,\n\u001b[1;32m    100\u001b[0m                 is_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    101\u001b[0m                 \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[1;32m    102\u001b[0m                 dict_factory\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m    103\u001b[0m                 retain_collection_types\u001b[38;5;241m=\u001b[39mretain_collection_types,\n\u001b[1;32m    104\u001b[0m                 value_serializer\u001b[38;5;241m=\u001b[39mvalue_serializer,\n\u001b[1;32m    105\u001b[0m             ),\n\u001b[1;32m    106\u001b[0m             _asdict_anything(\n\u001b[1;32m    107\u001b[0m                 vv,\n\u001b[1;32m    108\u001b[0m                 is_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m                 \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[1;32m    110\u001b[0m                 dict_factory\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m    111\u001b[0m                 retain_collection_types\u001b[38;5;241m=\u001b[39mretain_collection_types,\n\u001b[1;32m    112\u001b[0m                 value_serializer\u001b[38;5;241m=\u001b[39mvalue_serializer,\n\u001b[1;32m    113\u001b[0m             ),\n\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m kk, vv \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     rv[a\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m v\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/oic-scrape-D2fotO-x-py3.11/lib/python3.11/site-packages/attr/_funcs.py:106\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     95\u001b[0m     df \u001b[38;5;241m=\u001b[39m dict_factory\n\u001b[1;32m     96\u001b[0m     rv[a\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m df(\n\u001b[1;32m     97\u001b[0m         (\n\u001b[1;32m     98\u001b[0m             _asdict_anything(\n\u001b[1;32m     99\u001b[0m                 kk,\n\u001b[1;32m    100\u001b[0m                 is_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    101\u001b[0m                 \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[1;32m    102\u001b[0m                 dict_factory\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m    103\u001b[0m                 retain_collection_types\u001b[38;5;241m=\u001b[39mretain_collection_types,\n\u001b[1;32m    104\u001b[0m                 value_serializer\u001b[38;5;241m=\u001b[39mvalue_serializer,\n\u001b[1;32m    105\u001b[0m             ),\n\u001b[0;32m--> 106\u001b[0m             \u001b[43m_asdict_anything\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                \u001b[49m\u001b[43mis_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdict_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretain_collection_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_collection_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalue_serializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_serializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m kk, vv \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     rv[a\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m v\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/oic-scrape-D2fotO-x-py3.11/lib/python3.11/site-packages/attr/_funcs.py:124\u001b[0m, in \u001b[0;36m_asdict_anything\u001b[0;34m(val, is_key, filter, dict_factory, retain_collection_types, value_serializer)\u001b[0m\n\u001b[1;32m    120\u001b[0m             rv[a\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rv\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_asdict_anything\u001b[39m(\n\u001b[1;32m    125\u001b[0m     val,\n\u001b[1;32m    126\u001b[0m     is_key,\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mfilter\u001b[39m,\n\u001b[1;32m    128\u001b[0m     dict_factory,\n\u001b[1;32m    129\u001b[0m     retain_collection_types,\n\u001b[1;32m    130\u001b[0m     value_serializer,\n\u001b[1;32m    131\u001b[0m ):\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    ``asdict`` only works on attrs instances, this works on anything.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(val\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__attrs_attrs__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;66;03m# Attrs class.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nih = NihApi()\n",
    "\n",
    "result_list: Optional[pl.LazyFrame] = None\n",
    "\n",
    "total_projects = nih.count_projects_added_in_date_range(parameters[\"start_date\"], parameters[\"end_date\"])\n",
    "pbar = tqdm(total=total_projects, desc=f\"Projects from {parameters['start_date']} to {parameters['end_date']}\")\n",
    "date_spine = [\n",
    "    parameters[\"start_date\"] + timedelta(days=x)\n",
    "    for x in range((parameters[\"end_date\"] - parameters[\"start_date\"]).days)\n",
    "]\n",
    "for date in date_spine:\n",
    "    pbar.write(f\"Retrieving projects for {date}\")\n",
    "    projects = nih.get_projects_by_day(date=datetime.date(2014, 9, 28), add_timestamp_field=True)\n",
    "    \n",
    "\n",
    "    if len(projects) > 0: # process when there are projects\n",
    "        futuredf = []\n",
    "        for project in projects:\n",
    "\n",
    "            ai = structure_award_item(project)\n",
    "            pbar.update(1)\n",
    "\n",
    "            futuredf.append(asdict(ai))\n",
    "        if result_list is None:\n",
    "            result_list = pl.LazyFrame(futuredf)\n",
    "        else:\n",
    "            result_list = pl.concat(items = [result_list, pl.LazyFrame(futuredf)], how='vertical_relaxed')\n",
    "    else: # no projects for this day\n",
    "        pass \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list.sink_parquet('data/nih.gov_grants.parquet')\n",
    "result_list.sink_ndjson('data/nih.gov_grants.ndjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oic-scrape-D2fotO-x-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
